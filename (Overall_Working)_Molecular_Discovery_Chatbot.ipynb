{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDp88JfMrtLK",
        "outputId": "3045b0a9-91c4-402b-951e-c1bcdccd0d71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b> The overall file integrates the chatbot, tokenizer, LLM, visualization, and exported database subsystem to display a Streamlit chatbot running on localtunnel that can perform 2D and 3D visualizations of SMILES, push and pull chat history and visualizations from Firebase database, and perform inferences from the LLM. </b></h2>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "7hYjWURJ3WwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install required libraries"
      ],
      "metadata": {
        "id": "-o9dyfyq3TqD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBSQ0Ap_tjIH",
        "outputId": "e2424b90-0d85-455d-fd04-7f598d237a2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2023.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdkit-pypi\n",
            "  Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting py3dmol\n",
            "  Downloading py3Dmol-2.1.0-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyrebase\n",
            "  Downloading Pyrebase-3.0.27-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting streamlit\n",
            "  Downloading streamlit-1.33.0-py2.py3-none-any.whl (8.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting streamlit_chat\n",
            "  Downloading streamlit_chat-0.1.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair in /usr/local/lib/python3.10/dist-packages (4.2.2)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Collecting ctransformers\n",
            "  Downloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.25.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.5-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
            "  Downloading langchain_community-0.0.36-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
            "  Downloading langchain_core-0.1.48-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.52-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Collecting gcloud==0.17.0 (from pyrebase)\n",
            "  Downloading gcloud-0.17.0.tar.gz (458 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.0/458.0 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting oauth2client==3.0.0 (from pyrebase)\n",
            "  Downloading oauth2client-3.0.0.tar.gz (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.2/77.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycryptodome==3.4.3 (from pyrebase)\n",
            "  Downloading pycryptodome-3.4.3.tar.gz (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-jwt==2.0.1 (from pyrebase)\n",
            "  Downloading python_jwt-2.0.1-py2.py3-none-any.whl (8.8 kB)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.11.1-py2.py3-none-any.whl (514 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.8/514.8 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt==0.7.0 (from pyrebase)\n",
            "  Downloading requests_toolbelt-0.7.0-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from gcloud==0.17.0->pyrebase) (0.22.0)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.10/dist-packages (from gcloud==0.17.0->pyrebase) (1.63.0)\n",
            "Requirement already satisfied: protobuf!=3.0.0.b2.post1,>=3.0.0b2 in /usr/local/lib/python3.10/dist-packages (from gcloud==0.17.0->pyrebase) (3.20.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gcloud==0.17.0->pyrebase) (1.16.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client==3.0.0->pyrebase) (0.6.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client==3.0.0->pyrebase) (0.4.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client==3.0.0->pyrebase) (4.9)\n",
            "Collecting jws>=0.1.3 (from python-jwt==2.0.1->pyrebase)\n",
            "  Downloading jws-0.1.3.tar.gz (8.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "INFO: pip is looking at multiple versions of streamlit to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.32.2-py2.py3-none-any.whl (8.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging>=20.0 (from accelerate)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting streamlit\n",
            "  Downloading streamlit-1.32.1-py2.py3-none-any.whl (8.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading streamlit-1.32.0-py2.py3-none-any.whl (8.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading streamlit-1.31.1-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata<8,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (7.1.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
            "  Downloading streamlit-1.31.0-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading streamlit-1.30.0-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading streamlit-1.29.0-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata<7,>=1.4 (from streamlit)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "INFO: pip is looking at multiple versions of streamlit to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.28.2-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading streamlit-1.28.1-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading streamlit-1.28.0-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading streamlit-1.27.2-py2.py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading streamlit-1.27.1-py2.py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading streamlit-1.27.0-py2.py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading streamlit-1.26.0-py2.py3-none-any.whl (8.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pympler<2,>=0.9 (from streamlit)\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting streamlit\n",
            "  Downloading streamlit-1.25.0-py2.py3-none-any.whl (8.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading streamlit-1.24.1-py2.py3-none-any.whl (8.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich<14,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: toml<2 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Collecting tzlocal<5,>=1.1 (from streamlit)\n",
            "  Downloading tzlocal-4.3.1-py3-none-any.whl (20 kB)\n",
            "Collecting validators<1,>=0.2 (from streamlit)\n",
            "  Downloading validators-0.28.1-py3-none-any.whl (39 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.1.dev5 (from streamlit)\n",
            "  Downloading pydeck-0.9.0-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog (from streamlit)\n",
            "  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair) (0.12.1)\n",
            "INFO: pip is looking at multiple versions of tiktoken to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tiktoken-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tiktoken-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tiktoken-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of tiktoken to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading tiktoken-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blobfile>=2 (from tiktoken)\n",
            "  Downloading blobfile-2.1.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tiktoken-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tiktoken-0.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from ctransformers) (9.0.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting pycryptodomex~=3.8 (from blobfile>=2->tiktoken)\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from blobfile>=2->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: lxml~=4.9 in /usr/local/lib/python3.10/dist-packages (from blobfile>=2->tiktoken) (4.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit) (3.18.1)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair) (0.35.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair) (0.18.0)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.11.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.11.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting pytz-deprecation-shim (from tzlocal<5,>=1.1->streamlit)\n",
            "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2>=0.9.1->gcloud==0.17.0->pyrebase) (3.1.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.11.0->streamlit) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.8.3->langchain) (3.7)\n",
            "Building wheels for collected packages: gcloud, oauth2client, pycryptodome, jws\n",
            "  Building wheel for gcloud (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gcloud: filename=gcloud-0.17.0-py3-none-any.whl size=638001 sha256=0873e678c4fe96a1773efb70ad83c0eb48a6cede5798800963161c00742fc0d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/7f/0b/0bd775c9e7f572be68ff9d285c17f2c497e7176235f897cb20\n",
            "  Building wheel for oauth2client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oauth2client: filename=oauth2client-3.0.0-py3-none-any.whl size=106350 sha256=d9f816c62a7d34c837dbda5d4b56afc6805e0529d4a73598270515a5d5f7dfb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/da/55/05/62cddd6f3a70f9551e75efc49aafc0a4e91a77760fb83632d9\n",
            "  Building wheel for pycryptodome (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycryptodome: filename=pycryptodome-3.4.3-cp310-cp310-linux_x86_64.whl size=6834811 sha256=a89455d0ded44fcf4fbf3d0a2401891d1cadef6ce941b07144b4fc9f0ebcfe42\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/8d/f5/17e414ab6a18afa63e3a0c46a4912960edc6af44b7b70b68b6\n",
            "  Building wheel for jws (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jws: filename=jws-0.1.3-py3-none-any.whl size=9395 sha256=1a1340d0a001cf07e7ea40e716a33c43352d88833bbc5169888b8b26cb79f7df\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/f1/f9/add57b12d06f2d00045e86f6d5b2163aea577a8cf2f65c8367\n",
            "Successfully built gcloud oauth2client pycryptodome jws\n",
            "Installing collected packages: requests, pycryptodome, py3dmol, jws, watchdog, validators, smmap, requests-toolbelt, rdkit-pypi, rdkit, pytz-deprecation-shim, python-jwt, pypdf, pympler, pycryptodomex, packaging, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, jsonpointer, importlib-metadata, faiss-cpu, tzlocal, typing-inspect, pydeck, oauth2client, nvidia-cusparse-cu12, nvidia-cudnn-cu12, marshmallow, jsonpatch, gitdb, blobfile, tiktoken, nvidia-cusolver-cu12, langsmith, gitpython, gcloud, dataclasses-json, ctransformers, pyrebase, langchain-core, streamlit, sentence_transformers, langchain-text-splitters, langchain-community, accelerate, streamlit_chat, langchain\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.1.0\n",
            "    Uninstalling importlib_metadata-7.1.0:\n",
            "      Successfully uninstalled importlib_metadata-7.1.0\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 5.2\n",
            "    Uninstalling tzlocal-5.2:\n",
            "      Successfully uninstalled tzlocal-5.2\n",
            "  Attempting uninstall: oauth2client\n",
            "    Found existing installation: oauth2client 4.1.3\n",
            "    Uninstalling oauth2client-4.1.3:\n",
            "      Successfully uninstalled oauth2client-4.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydrive 1.3.1 requires oauth2client>=4.0.0, but you have oauth2client 3.0.0 which is incompatible.\n",
            "pydrive2 1.6.3 requires oauth2client>=4.0.0, but you have oauth2client 3.0.0 which is incompatible.\n",
            "bigframes 1.3.0 requires requests>=2.27.1, but you have requests 2.11.1 which is incompatible.\n",
            "cachecontrol 0.14.0 requires requests>=2.16.0, but you have requests 2.11.1 which is incompatible.\n",
            "google-api-core 2.11.1 requires requests<3.0.0.dev0,>=2.18.0, but you have requests 2.11.1 which is incompatible.\n",
            "google-cloud-bigquery 3.21.0 requires requests<3.0.0dev,>=2.21.0, but you have requests 2.11.1 which is incompatible.\n",
            "google-cloud-storage 2.8.0 requires requests<3.0.0dev,>=2.18.0, but you have requests 2.11.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.11.1 which is incompatible.\n",
            "pandas-datareader 0.10.0 requires requests>=2.19.0, but you have requests 2.11.1 which is incompatible.\n",
            "pooch 1.8.1 requires requests>=2.19.0, but you have requests 2.11.1 which is incompatible.\n",
            "spacy 3.7.4 requires requests<3.0.0,>=2.13.0, but you have requests 2.11.1 which is incompatible.\n",
            "tensorboard 2.15.2 requires requests<3,>=2.21.0, but you have requests 2.11.1 which is incompatible.\n",
            "tensorflow-datasets 4.9.4 requires requests>=2.19.0, but you have requests 2.11.1 which is incompatible.\n",
            "tweepy 4.14.0 requires requests<3,>=2.27.0, but you have requests 2.11.1 which is incompatible.\n",
            "weasel 0.3.4 requires requests<3.0.0,>=2.13.0, but you have requests 2.11.1 which is incompatible.\n",
            "yfinance 0.2.38 requires requests>=2.31, but you have requests 2.11.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.29.3 blobfile-2.1.1 ctransformers-0.2.27 dataclasses-json-0.6.5 faiss-cpu-1.8.0 gcloud-0.17.0 gitdb-4.0.11 gitpython-3.1.43 importlib-metadata-6.11.0 jsonpatch-1.33 jsonpointer-2.4 jws-0.1.3 langchain-0.1.16 langchain-community-0.0.36 langchain-core-0.1.48 langchain-text-splitters-0.0.1 langsmith-0.1.52 marshmallow-3.21.1 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 oauth2client-3.0.0 orjson-3.10.1 packaging-23.2 py3dmol-2.1.0 pycryptodome-3.4.3 pycryptodomex-3.20.0 pydeck-0.9.0 pympler-1.0.1 pypdf-4.2.0 pyrebase-3.0.27 python-jwt-2.0.1 pytz-deprecation-shim-0.1.0.post0 rdkit-2023.9.6 rdkit-pypi-2022.9.5 requests-2.11.1 requests-toolbelt-0.7.0 sentence_transformers-2.7.0 smmap-5.0.1 streamlit-1.24.1 streamlit_chat-0.1.1 tiktoken-0.1.1 typing-inspect-0.9.0 tzlocal-4.3.1 validators-0.28.1 watchdog-4.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch pathlib stmol rdkit rdkit-pypi py3dmol pillow langchain torch accelerate pyrebase transformers sentence_transformers streamlit streamlit_chat faiss-cpu altair tiktoken huggingface-hub ctransformers pandas pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLsaCFQVL5ME"
      },
      "source": [
        "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/tree/main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaB_6ujtwv1X"
      },
      "source": [
        "## Install localtunnel to serve the Streamlit app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNSLBKelwuZY",
        "outputId": "bda92ca5-2b05-4f95-deb9-29d14fe56140"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 3.625s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel\n",
        "# Look into githubio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qI5M-DSeJDm",
        "outputId": "c857e87c-630d-46bc-9c72-35852e8ce5a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: py3Dmol in /usr/local/lib/python3.10/dist-packages (2.1.0)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (2023.9.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n"
          ]
        }
      ],
      "source": [
        "%run '/content/drive/MyDrive/Colab Notebooks/Data_Visualization_Molecular_Discovery_Chatbot.ipynb'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-AwhTVL01Dy"
      },
      "source": [
        "## Write ALL code to one file (app.py) for execution on localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjabZ0Q5sVJH",
        "outputId": "c1314e2f-daa7-4a16-acb6-ee9558737e1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "## Adapted Visualization Code from https://github.com/karthick1087/SMILES-to-PDB-Converter-and-3D-Visualizer/blob/main/app.py\n",
        "\n",
        "# Import required libraries and modules\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import CTransformers\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "### Utilfunctions.py\n",
        "import pandas as pd\n",
        "import secrets\n",
        "import string\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors\n",
        "import py3Dmol\n",
        "import streamlit as st\n",
        "import os\n",
        "import tempfile\n",
        "import base64\n",
        "\n",
        "def smiles_to_pdb(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None, None\n",
        "    mol = Chem.AddHs(mol)\n",
        "    AllChem.EmbedMolecule(mol, AllChem.ETKDG())\n",
        "    AllChem.UFFOptimizeMolecule(mol)\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdb\") as temp_pdb:\n",
        "        Chem.MolToPDBFile(mol, temp_pdb.name)\n",
        "        return temp_pdb.name, mol\n",
        "\n",
        "# Function to generate 2D structure from SMILES notation\n",
        "def generate_2d_structure(smiles):\n",
        "    # Convert SMILES to RDKit molecule object\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    # Add hydrogen atoms to the molecule\n",
        "    mol = Chem.AddHs(mol)\n",
        "    # Compute 2D coordinates for the molecule\n",
        "    AllChem.Compute2DCoords(mol)\n",
        "    return mol\n",
        "\n",
        "from rdkit.Chem import Draw\n",
        "\n",
        "# Function to visualize 2D structure using py3Dmol\n",
        "def visualize_2d_structure(smiles):\n",
        "    molecule_mol = generate_2d_structure(smiles)\n",
        "    fp = '/content/cdk2_mol1.png'\n",
        "    Draw.MolToFile(molecule_mol, fp)\n",
        "    return fp\n",
        "\n",
        "# Generate Mol File describing 3D Structure from SMILES notation\n",
        "def generate_3d_structure(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol:\n",
        "        mol = Chem.AddHs(mol)\n",
        "        AllChem.EmbedMolecule(mol, randomSeed=42)\n",
        "        pdb_data = Chem.MolToPDBBlock(mol)\n",
        "        return pdb_data\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "from stmol import *\n",
        "\n",
        "# Visualize 3D structure from file\n",
        "def visualize_3d_structure(smiles):\n",
        "    pdb_data = generate_3d_structure(smiles)\n",
        "    if pdb_data:\n",
        "        # Create py3Dmol view\n",
        "        view = py3Dmol.view(width=800, height=400)\n",
        "        view.addModel(pdb_data, 'pdb')\n",
        "        # Set style and display\n",
        "        view.setStyle({'cartoon': {'color': 'spectrum'}})\n",
        "        view.zoomTo()\n",
        "\n",
        "    else:\n",
        "        st.write(\"No 3D structure available.\")\n",
        "\n",
        "\n",
        "def randomString():\n",
        "    return ''.join(secrets.choice(string.ascii_letters) for _ in range(6))\n",
        "\n",
        "\n",
        "def print_stats(data):\n",
        "    series = pd.Series(data)\n",
        "    # Use describe() to get statistics\n",
        "    statistics_series = series.describe()\n",
        "    # Display the statistics Series\n",
        "    print(statistics_series)\n",
        "\n",
        "###\n",
        "\n",
        "# Function to tokenize text from PDF documents and create embeddings\n",
        "def textTokenizer():\n",
        "    # Load PDF files from the specified path\n",
        "    loader = DirectoryLoader('/content/drive/MyDrive/LLama2HealthCareChatBot-master/data/', glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split text into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=10)\n",
        "    text_chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Create embeddings storing semantic information\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                                       model_kwargs={'device': \"cpu\"})\n",
        "\n",
        "    # Vectorstore for fast similarity search via indexing\n",
        "    vector_store = FAISS.from_documents(text_chunks, embeddings)\n",
        "\n",
        "    return vector_store\n",
        "\n",
        "# Function to create a conversational retrieval chain model\n",
        "def createModel(temperature, kval, maxTokens):\n",
        "    vector_store = textTokenizer()\n",
        "\n",
        "    # Load Huggingface Llama2 LLM with specified hyperparmaters\n",
        "    llm = CTransformers(model=\"/content/drive/MyDrive/LLama2HealthCareChatBot-master/llama-2-7b-chat.ggmlv3.q4_0.bin\", model_type=\"llama\",\n",
        "                        config={'max_new_tokens': 128, 'temperature': 0.01}, n_ctx=4096)\n",
        "\n",
        "    # Create memory object to store chat history\n",
        "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "    # Set up conversational chain that connects LLM, the indexed vectorized data, and the chatbot\n",
        "    chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm, chain_type='stuff',\n",
        "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 2}),  # k hyperparameter\n",
        "        memory=memory)\n",
        "\n",
        "    return chain\n",
        "\n",
        "# Import required libraries and modules\n",
        "import streamlit as st\n",
        "from streamlit_chat import message\n",
        "\n",
        "# Function to handle the conversation with the chatbot\n",
        "def conversation_chat(query, chain):\n",
        "    result = chain({\"question\": query, \"chat_history\": st.session_state['history']})\n",
        "    st.session_state['history'].append((query, result[\"answer\"]))\n",
        "    return result[\"answer\"]\n",
        "\n",
        "# Function to initialize Hey and Hello salutations at the initiation of chatbot\n",
        "def initialize_session_state():\n",
        "    if 'history' not in st.session_state:\n",
        "        st.session_state['history'] = []\n",
        "\n",
        "    if 'generated' not in st.session_state:\n",
        "        st.session_state['generated'] = [\"Hello! Ask me anything about proteins\"]\n",
        "\n",
        "    if 'past' not in st.session_state:\n",
        "        st.session_state['past'] = [\"Hey! 👋\"]\n",
        "\n",
        "\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials, firestore, storage\n",
        "\n",
        "# Upload PNG file to Firebase Storage\n",
        "def upload_to_storage(local_file_path, remote_file_name):\n",
        "    bucket = storage.bucket()\n",
        "    blob = bucket.blob(remote_file_name)\n",
        "    blob.upload_from_filename(local_file_path)\n",
        "\n",
        "    # Get download URL\n",
        "    download_url = blob.generate_signed_url(expiration=3600)  # URL expires in 1 hour\n",
        "    return download_url\n",
        "\n",
        "# Function to encode MolViewSpec data\n",
        "def encode_mvs_data(mvs_data):\n",
        "    return base64.urlsafe_b64encode(mvs_data.encode()).decode('utf-8')\n",
        "\n",
        "# Function to display chat history and handle user input\n",
        "def display_chat_history(chain):\n",
        "    reply_container = st.container()\n",
        "    container = st.container()\n",
        "\n",
        "    firebase_initialized = False\n",
        "\n",
        "    with container:\n",
        "\n",
        "        # Request user for prompt\n",
        "        with st.form(key='my_form', clear_on_submit=True):\n",
        "            user_input = st.text_input(\"Question:\", placeholder=\"Ask any protein question\", key='input')\n",
        "\n",
        "            # Create send button to submit prompt\n",
        "            submit_button = st.form_submit_button(label='Send')\n",
        "\n",
        "        # pipe = create_new_model()\n",
        "\n",
        "        # When input prompt is valid withand update session states respectively\n",
        "        if submit_button and user_input:\n",
        "            with st.spinner(\"Fetching response...\"):\n",
        "                    # Call conversational retrieval chain to produce response from LLM using previous chat history\n",
        "                    #result = pipe(user_input, do_sample=True)\n",
        "                    #(result[0]['generated_text'])\n",
        "                    output = conversation_chat(user_input, chain)\n",
        "\n",
        "            # Update input and output session states with current prompt and responses\n",
        "            st.session_state['past'].append(user_input)\n",
        "            st.session_state['generated'].append(output)\n",
        "\n",
        "        # Create export button to export to database\n",
        "        export_button = st.button(label='Export')\n",
        "\n",
        "        if not firebase_admin._apps:  # Check if Firebase Admin SDK has not been initialized\n",
        "            cred_obj = credentials.Certificate('/content/drive/MyDrive/LLama2HealthCareChatBot-master/serviceAccountKey.json') # REPLACE WITH OWN SERVICE ACCOUNT KEY FROM FIREBASE\n",
        "            firebase_admin.initialize_app(cred_obj, {\n",
        "                'databaseURL': \"https://molecular-discovery-chatbot-default-rtdb.firebaseio.com/\",\n",
        "                'storageBucket': \"molecular-discovery-chatbot.appspot.com\"\n",
        "            })\n",
        "\n",
        "        db = firestore.client()\n",
        "        ref = db.collection('chatHistory')\n",
        "        # When user wants to export to database\n",
        "        if export_button:\n",
        "\n",
        "            # Get the chat history\n",
        "            chat_history = st.session_state['history']\n",
        "\n",
        "            # Convert chat history to a format suitable for Firebase (e.g., list of dictionaries)\n",
        "            firebase_data = [{'user': message[0], 'bot': message[1]} for message in chat_history]\n",
        "\n",
        "\n",
        "            for data in firebase_data:\n",
        "                ref.collection('chatHistory').add(data)\n",
        "\n",
        "        smiles_input = st.text_input(\"Enter SMILES to visualize:\")\n",
        "        if st.button(\"2D\"):\n",
        "            file_path = visualize_2d_structure(smiles_input)\n",
        "            st.image(file_path)\n",
        "\n",
        "        if st.button(\"Export 2D\"):\n",
        "            file_path = visualize_2d_structure(smiles_input)\n",
        "            if file_path is not None:\n",
        "                remote_file_name = \"images/smiles_input.png\"\n",
        "                download_url = upload_to_storage(file_path, remote_file_name)\n",
        "                st.write(download_url)\n",
        "                # Save URL to Firebase Database\n",
        "                # doc_id = save_to_database(db, download_url)\n",
        "                st.write(\"Uploaded successfully\")\n",
        "\n",
        "        if st.button(\"3D\"):\n",
        "            pdb_file, mol = smiles_to_pdb(smiles_input)\n",
        "            if pdb_file is not None:\n",
        "                st.success(\"Conversion successful! PDB file generated.\")\n",
        "                st.write(\"### 3D Visualization:\")\n",
        "                with open(pdb_file, 'rb') as f:\n",
        "                    pdb_data = f.read()\n",
        "                encoded_pdb_data = base64.b64encode(pdb_data).decode('utf-8')\n",
        "\n",
        "                # Construct MolViewSpec data\n",
        "                mvs_data = \"\"\"\n",
        "                {\n",
        "                    \"metadata\": {\n",
        "                        \"title\": \"Molecule Visualization\",\n",
        "                        \"version\": \"1\",\n",
        "                        \"timestamp\": \"2024-04-25T12:00:00\"\n",
        "                    },\n",
        "                    \"root\": {\n",
        "                        \"kind\": \"root\",\n",
        "                        \"children\": [\n",
        "                            {\n",
        "                                \"kind\": \"download\",\n",
        "                                \"params\": {\n",
        "                                    \"url\": \"data:chemical/x-pdb;base64,\" + \"%s\"\n",
        "                                }\n",
        "                            }\n",
        "                        ]\n",
        "                    }\n",
        "                }\n",
        "                \"\"\" % encoded_pdb_data\n",
        "\n",
        "                # Encode MolViewSpec data\n",
        "                encoded_mvs_data = encode_mvs_data(mvs_data)\n",
        "\n",
        "                # Construct MolView URL\n",
        "                link = f\"https://molstar.org/viewer?mvs-format=mvsj&mvs-data={encoded_mvs_data}\"\n",
        "                st.markdown(f'[View 3D Visualization]({link})')\n",
        "\n",
        "                st.write(\"### Download PDB file:\")\n",
        "                with open(pdb_file, \"rb\") as f:\n",
        "                  pdb_bytes = f.read()\n",
        "                st.download_button(\n",
        "                    label=\"Download PDB file\",\n",
        "                    data=pdb_bytes,\n",
        "                    file_name=\"molecule.pdb\",\n",
        "                    mime=\"chemical/x-pdb\"\n",
        "                )\n",
        "                st.write(\"###\")\n",
        "\n",
        "        if st.button(\"Export 3D\"):\n",
        "            pass\n",
        "\n",
        "\n",
        "\n",
        "    # UI development to generate emojis and bubbles when new response generated\n",
        "    if st.session_state['generated']:\n",
        "        with reply_container:\n",
        "            for i in range(len(st.session_state['generated'])):\n",
        "                message(st.session_state[\"past\"][i], is_user=True, key=str(i) + '_user', avatar_style=\"thumbs\")\n",
        "                message(st.session_state[\"generated\"][i], key=str(i), avatar_style=\"fun-emoji\")\n",
        "\n",
        "file_path = None\n",
        "# Main function to run the Streamlit app\n",
        "def main():\n",
        "    # Define the current page based on query parameters\n",
        "    current_page = st.experimental_get_query_params().get(\"page\", [\"Landing\"])[0]\n",
        "\n",
        "    # On chatbot page\n",
        "    if current_page == \"Chatbot\":\n",
        "\n",
        "        # UI work displaying titles\n",
        "        st.title(\"Molecular Discovery Chatbot 🧑🏽‍⚕️\")\n",
        "\n",
        "        # Initialize Hey and Hello message\n",
        "        initialize_session_state()\n",
        "\n",
        "        # Create Model\n",
        "        chain1 = createModel(0.5, 2, 256)\n",
        "\n",
        "        # Display chat history\n",
        "        display_chat_history(chain1)\n",
        "\n",
        "        # Create navigation to database tab button\n",
        "        st.button(\"Go to Database\", on_click=lambda: st.experimental_set_query_params(page=\"Database\"))\n",
        "\n",
        "\n",
        "    elif current_page == \"Database\":\n",
        "\n",
        "        # UI work displaying titles\n",
        "        st.header(\"Firebase Data\")\n",
        "        st.write(\"Molecule Database in a table:\")\n",
        "\n",
        "        # Create Fetch button to retrieve data from firebase\n",
        "        fetch_button = st.button(\"Fetch Database\")\n",
        "\n",
        "        # When clicked\n",
        "        if fetch_button:\n",
        "            # Retrieve relevant data from firebase as key, vlue pairs\n",
        "            firebase_sample_data = ref.reference('/bookData').get()\n",
        "            chat_history_data = ref.reference('/chatHistory').get()\n",
        "\n",
        "            # Output respective data in tabular format by deconstructing the pairs\n",
        "            if firebase_sample_data:\n",
        "                data_list = [{key: value} for key, value in firebase_sample_data.items()]\n",
        "                st.table(data_list)\n",
        "            if chat_history_data:\n",
        "                data_list = [{'User': message['user'], 'Bot': message['bot']} for message in chat_history_data]\n",
        "                st.table(data_list)\n",
        "        else:\n",
        "            st.write(\"No data available in firebase\")\n",
        "        # Create navigation to chatbot tab button\n",
        "        st.button(\"Go to Chatbot\", on_click=lambda: st.experimental_set_query_params(page=\"Chatbot\"))\n",
        "    else:\n",
        "        # UI for landing page\n",
        "        st.title(\"Welcome to Molecular Discovery Chatbot\")\n",
        "        st.write(\"Pick an option\")\n",
        "\n",
        "        # Create navigation to chatbot and database tab from landing page\n",
        "        st.button(\"Go to Chatbot\", on_click=lambda: st.experimental_set_query_params(page=\"Chatbot\"))\n",
        "        st.button(\"Go to Database\", on_click=lambda: st.experimental_set_query_params(page=\"Database\"))\n",
        "\n",
        "# Run the main function to start the Streamlit app\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74UlSE5IxJ-M"
      },
      "source": [
        "## Run the Streamlit app in the background"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pVuqDUKtggm"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s9ceUkr2A_I"
      },
      "source": [
        "## Retrieve external URL public Ipv4 for local webserver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1B2Uzw1xpOE",
        "outputId": "0e8cb8e1-030f-411b-9820-962484ee8f48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Password/Enpoint IP for localtunnel is: 34.86.245.151\n"
          ]
        }
      ],
      "source": [
        "# Also found as external URL in log.txt\n",
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkZ3uCsexLDb"
      },
      "source": [
        "## Expose the Streamlit app on port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIjluplYvbJK",
        "outputId": "a54137bf-ff83-4a34-9696-eb2c5185b545"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.86.245.151\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 4.442s\n",
            "your url is: https://tender-buckets-cover.loca.lt\n"
          ]
        }
      ],
      "source": [
        "# https://theboroer.github.io/localtunnel-www/\n",
        "!npx localtunnel --port 8501 & curl ipv4.icanhazip.com\n",
        "\n",
        "# 5 min 52 seconds with T4 GPU\n",
        "# 5 min 27 seconds with V100 GPU\n",
        "# Improvement from 6 minute 30 seconds with CPU but workload is still highly unparallelized\n",
        "# We barely use 0.7 / 16 GB GPU RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFLkYmeIPNGV"
      },
      "outputs": [],
      "source": [
        "# 3D Visualization Credit: https://molstar.org/viewer-docs/extensions/mvs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "d0WDL4FmxRZl",
        "outputId": "7491b705-8ea9-4770-f6c1-ab229710a22a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nWhat happened before/after fetching response (the delay)\\n\\nhttps://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore\\nhttps://api.python.langchain.com/en/latest/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html\\n\\nWe could explore smaller LLMs.\\nHave a few benchmark questions.\\nQuantify that migration to Google Colab did not significantly affect chatbot response. Craft a hypothesis (communication overhead, etc.) and identify bottlenecks (at different stages)\\nEvaluate amount of time each component took.\\nValidate response of specific questions and assess how the input PDF data contributes to response (without aspirin / with aspirin)\\nAre there specific questions that the default model cannot answer but our model does?\\n\\nPresentation\\n- What are the conclusions from the results?\\n- What are the actions going forward?\\n\\nThere are many sources of GPU computing power (idle time etc.) and what constraints we have for the project.\\nCould we run this on Grace cluster, considering queue time?\\n'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "What happened before/after fetching response (the delay)\n",
        "\n",
        "https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore\n",
        "https://api.python.langchain.com/en/latest/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html\n",
        "\n",
        "We could explore smaller LLMs.\n",
        "Have a few benchmark questions.\n",
        "Quantify that migration to Google Colab did not significantly affect chatbot response. Craft a hypothesis (communication overhead, etc.) and identify bottlenecks (at different stages)\n",
        "Evaluate amount of time each component took.\n",
        "Validate response of specific questions and assess how the input PDF data contributes to response (without aspirin / with aspirin)\n",
        "Are there specific questions that the default model cannot answer but our model does?\n",
        "\n",
        "Presentation\n",
        "- What are the conclusions from the results?\n",
        "- What are the actions going forward?\n",
        "\n",
        "There are many sources of GPU computing power (idle time etc.) and what constraints we have for the project.\n",
        "Could we run this on Grace cluster, considering queue time?\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}