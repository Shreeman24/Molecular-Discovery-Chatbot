{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpG4GF0v2_b-",
        "outputId": "8b8e7d88-b695-4b7a-98c4-edee49687e31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWG8mu-UZABV"
      },
      "source": [
        "<h2><b> Prompt Engineering assesses the LLM on binding affinity label prediction based on SMILES and Protein Sequence. The labels are outputted to a file from which accuracy and validity scores can be calculated. </b></h2>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Prompts"
      ],
      "metadata": {
        "id": "s7xAQzJeSqGU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No-H8obctuaE"
      },
      "outputs": [],
      "source": [
        "def create_binding_affinity_prompt(examples):\n",
        "    prompt = \"\"\"[INST] <<SYS>>\n",
        "    You are an expert chemist, your task is to predict the binding affinity (Label), given a compound (SMILES) and\n",
        "    given protein sequence (Sequence) using your experienced chemical property prediction knowledge.\n",
        "    Please strictly follow the format, no other information can be provided.\n",
        "    Given the SMILES string of compound and Protein Sequence, predict the binding affinity based on training compound-protein pair dataset.\n",
        "    Please answer with one word: High, Medium, Low corresponding to binding affinity label.\n",
        "    <</SYS>>\\n\"\"\"\n",
        "    count = 0\n",
        "    for i in range(len(examples)):\n",
        "        prompt += f\"SMILES: {examples[i][0]}\\nSequence: {examples[i][1]}\\n[/INST]\\nLabel: {examples[i][2]}\\n\"\n",
        "        if (i != (len(examples) - 1)): # Last example does not need to append INST\n",
        "          prompt += \"[INST]\\n\"\n",
        "    return prompt\n",
        "\n",
        "def create_bace_prompt(input_smiles, pp_examples):\n",
        "    prompt = \"You are an expert chemist tasked with predicting molecule properties based on chemical structure. Given a molecule's SMILES string, predict if it inhibits (Yes) the Beta-site Amyloid Precursor Protein Cleaving Enzyme 1 (BACE1) or not (No) and provide response as Yes or No.\"\n",
        "    for example in pp_examples:\n",
        "        prompt += f\"SMILES: {example[0]}\\nBACE-1 Inhibit: {example[-1]}\\n\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Train and Test for Binding Affinity Data"
      ],
      "metadata": {
        "id": "62GZBvJGSsiN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqMPjdy6s9Kd"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Random sampling\n",
        "def generate_train_random(binding_affinity_train, sample_size):\n",
        "    high_values = binding_affinity_train[binding_affinity_train[\"Label\"] == 'High'].sample(sample_size)\n",
        "    medium_values = binding_affinity_train[binding_affinity_train[\"Label\"] == 'Medium'].sample(sample_size)\n",
        "    low_values = binding_affinity_train[binding_affinity_train[\"Label\"] == 'Low'].sample(sample_size)\n",
        "\n",
        "    print(\"Parsing SMILES, Sequence, Label\")\n",
        "\n",
        "    high_values_smiles = high_values[\"SMILES\"].to_list()\n",
        "    medium_values_smiles = medium_values[\"SMILES\"].to_list()\n",
        "    low_values_smiles = low_values[\"SMILES\"].to_list()\n",
        "\n",
        "    high_values_proteins = high_values[\"Sequence\"].to_list()\n",
        "    medium_values_proteins = medium_values[\"Sequence\"].to_list()\n",
        "    low_values_proteins = low_values[\"Sequence\"].to_list()\n",
        "\n",
        "    high_values_affinity = high_values[\"Label\"].to_list()\n",
        "    medium_values_affinity = medium_values[\"Label\"].to_list()\n",
        "    low_values_affinity = low_values[\"Label\"].to_list()\n",
        "\n",
        "    print(\"Successfully Parsed\")\n",
        "\n",
        "    sampled_list = []\n",
        "    for i in range(len(high_values_smiles)):\n",
        "      sampled_list += [[high_values_smiles[i], high_values_proteins[i], high_values_affinity[i]]]\n",
        "    for i in range(len(medium_values_smiles)):\n",
        "      sampled_list += [[medium_values_smiles[i], medium_values_proteins[i], medium_values_affinity[i]]]\n",
        "    for i in range(len(low_values_smiles)):\n",
        "      sampled_list += [[low_values_smiles[i], low_values_proteins[i], low_values_affinity[i]]]\n",
        "\n",
        "    random.shuffle(sampled_list)\n",
        "\n",
        "    return sampled_list\n",
        "\n",
        "def generate_test_random(binding_affinity_test, sample_size):\n",
        "    sampled_rows = binding_affinity_test.sample(n=sample_size, random_state=42)\n",
        "    sampled_rows_list = sampled_rows.values.tolist()\n",
        "\n",
        "    return sampled_rows_list\n",
        "\n",
        "def generate_test_proportional(binding_affinity_test, sample_size):\n",
        "    pass;"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduce Noise"
      ],
      "metadata": {
        "id": "tFqKPaQgSv8C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyhOF-Ekdlt6"
      },
      "outputs": [],
      "source": [
        "def add_noise(data, x):\n",
        "    \"\"\"Makes X decimal of samples have their x-y flipped to add noise to data\"\"\"\n",
        "    sample_size = int(x * len(data))\n",
        "    noise_samples = data.sample(sample_size)\n",
        "\n",
        "    # Generate random indices for swapping\n",
        "    indices_to_swap = np.random.choice(len(data), size=sample_size, replace=False)\n",
        "\n",
        "    # Swap elements in columns\n",
        "    data.loc[indices_to_swap, ['SMILES', 'Sequence']], data.loc[indices_to_swap, ['Sequence', 'SMILES']] = data.loc[indices_to_swap, ['Sequence', 'SMILES']].values, data.loc[indices_to_swap, ['SMILES', 'Sequence']].values\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6w_vSr8zX4j"
      },
      "source": [
        "## Generate train and test with fixed sample size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPE1DEq1zXpb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def create_train_test_list(train_size, test_size):\n",
        "    print(\"Entering function\")\n",
        "    train_data = pd.read_csv('/content/drive/MyDrive/LLama2HealthCareChatBot-master/data/BindingAffinity_data/binding_affinity_train.csv').rename(columns={'Canonical SMILE': 'SMILES'})\n",
        "    print(\"Read train data\")\n",
        "    test_data = pd.read_csv('/content/drive/MyDrive/LLama2HealthCareChatBot-master/data/BindingAffinity_data/binding_affinity_test.csv').rename(columns={'Canonical SMILE': 'SMILES'})\n",
        "\n",
        "    train_sample_size = train_size\n",
        "    test_sample_size = test_size\n",
        "\n",
        "    print(\"Creating lists\")\n",
        "\n",
        "    train_list = generate_train_random(train_data, train_size)\n",
        "    test_list = generate_test_random(test_data, test_size)\n",
        "\n",
        "    return train_list, test_list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install required libraries"
      ],
      "metadata": {
        "id": "O3OF6t6FXuwe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHn7koCY37HV",
        "outputId": "03bac96e-bd1d-45c4-8bc9-a4b008c6b167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting CTransformers\n",
            "  Downloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unstructured\n",
            "  Downloading unstructured-0.13.6-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.5-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
            "  Downloading langchain_community-0.0.34-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
            "  Downloading langchain_core-0.1.47-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.52-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from CTransformers) (9.0.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.11.1-py2.py3-none-any.whl (433 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.11.0)\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.22.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2023.6.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting packaging>=20.0 (from transformers)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
            "Collecting deepdiff>=6.0 (from unstructured-client->unstructured)\n",
            "  Downloading deepdiff-7.0.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpath-python>=1.0.6 (from unstructured-client->unstructured)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Collecting mypy-extensions>=1.0.0 (from unstructured-client->unstructured)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting pypdf>=4.0 (from unstructured-client->unstructured)\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (2.8.2)\n",
            "Collecting ordered-set<4.2.0,>=4.1.0 (from deepdiff>=6.0->unstructured-client->unstructured)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=5c5efea77d4d4b17f3f37742414c0d695e4547b5bbdfaabedea554267b274d88\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, rapidfuzz, python-magic, python-iso639, pypdf, packaging, orjson, ordered-set, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, langdetect, jsonpointer, jsonpath-python, faiss-cpu, emoji, backoff, typing-inspect, nvidia-cusparse-cu12, nvidia-cudnn-cu12, marshmallow, jsonpatch, deepdiff, nvidia-cusolver-cu12, langsmith, dataclasses-json, CTransformers, unstructured-client, langchain-core, unstructured, sentence-transformers, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed CTransformers-0.2.27 backoff-2.2.1 dataclasses-json-0.6.5 deepdiff-7.0.1 emoji-2.11.1 faiss-cpu-1.8.0 filetype-1.2.0 jsonpatch-1.33 jsonpath-python-1.0.6 jsonpointer-2.4 langchain-0.1.16 langchain-community-0.0.34 langchain-core-0.1.47 langchain-text-splitters-0.0.1 langdetect-1.0.9 langsmith-0.1.52 marshmallow-3.21.1 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 ordered-set-4.1.0 orjson-3.10.1 packaging-23.2 pypdf-4.2.0 python-iso639-2024.4.27 python-magic-0.4.27 rapidfuzz-3.8.1 sentence-transformers-2.7.0 typing-inspect-0.9.0 unstructured-0.13.6 unstructured-client-0.22.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain CTransformers unstructured sentence-transformers faiss-cpu transformers pathlib huggingface-hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create random labels based on test data frequency"
      ],
      "metadata": {
        "id": "j_Voyyjq2A1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def random_model_prompt(test_count):\n",
        "  \"\"\"Generates a test prompt with random labels that accounts for probability of occurrences in test data\"\"\"\n",
        "  # Count number of occurrences of each label\n",
        "\n",
        "  train_data = pd.read_csv('/content/drive/MyDrive/LLama2HealthCareChatBot-master/data/BindingAffinity_data/binding_affinity_train.csv').rename(columns={'Canonical SMILE': 'SMILES'})\n",
        "\n",
        "  label_counts = train_data[\"Label\"].value_counts()\n",
        "  probabilities = label_counts / len(train_data)\n",
        "\n",
        "  print(probabilities)\n",
        "\n",
        "  probabilities_list = probabilities.to_list()\n",
        "  labels = [\"Medium\", \"Low\", \"High\"]\n",
        "\n",
        "  test_data = pd.read_csv('/content/drive/MyDrive/LLama2HealthCareChatBot-master/data/BindingAffinity_data/binding_affinity_test.csv').rename(columns={'Canonical SMILE': 'SMILES'})\n",
        "\n",
        "  X_test = test_data.drop('Label', axis=1)\n",
        "\n",
        "  test_list = generate_test_random(X_test, test_count)\n",
        "\n",
        "  entries = np.random.choice(labels, size=test_count, p=probabilities_list)\n",
        "\n",
        "  output_list = []\n",
        "\n",
        "  iterator = 0\n",
        "  test_prompt = \"You are an expert chemist, your task is to predict the binding affinity (Label), given a compound (SMILES) and given protein sequence (Sequence) using your experienced chemical property prediction knowledge. Please strictly follow the format, no other information can be provided. Given the SMILES string of compound and Protein Sequence, predict the binding affinity based on training compound-protein pair dataset. Please answer with one word: High, Medium, Low corresponding to binding affinity label.\\n\"\n",
        "  for test_example in test_list:\n",
        "    test_prompt += f\"SMILES: {test_example[1]}\\nSequence: {test_example[0]}\\nLabel:{entries[iterator]}\\n\"\n",
        "    output_list += [[test_example[1], test_example[0], entries[iterator]]]\n",
        "    iterator +=1\n",
        "\n",
        "  return output_list, test_prompt\n",
        "\n",
        "random_list, random_prompt = random_model_prompt(100)"
      ],
      "metadata": {
        "id": "9L4L16lCXwmc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5cb99fc-ea9c-4484-8748-3f572c3524e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "Medium    0.549768\n",
            "Low       0.393653\n",
            "High      0.056580\n",
            "Name: count, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot accuracy of labels against test sample size"
      ],
      "metadata": {
        "id": "htCoGRGV2E3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot accuracies with increasing test sample size\n",
        "def accuracy_graph(test_sample_size_list):\n",
        "    accuracy_list = []\n",
        "    for test_size in test_sample_size_list:\n",
        "        random_list, random_prompt = random_model_prompt(test_size)\n",
        "        total_count = len(random_list)  # Update total count for each test size\n",
        "        accuracy = random_accuracy_calculator(random_list, total_count)\n",
        "        accuracy_list.append(accuracy)\n",
        "\n",
        "    # Plotting\n",
        "    plt.plot(test_sample_size_list, accuracy_list)\n",
        "\n",
        "    # Adding labels and legend\n",
        "    plt.xlabel('Test Sample Size')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Plot of Accuracies with Increasing Test Sample Size')\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "def random_accuracy_calculator(predictions, total_count):\n",
        "    correct_prediction_count = 0  # Move this outside the loop to accumulate correct predictions\n",
        "    test_data = pd.read_csv('/content/drive/MyDrive/LLama2HealthCareChatBot-master/data/BindingAffinity_data/binding_affinity_test.csv').rename(columns={'Canonical SMILE': 'SMILES'})\n",
        "    for entry in predictions:\n",
        "        matched_row = test_data[(test_data[\"Sequence\"] == entry[0]) &\n",
        "                                (test_data[\"SMILES\"] == entry[1])]\n",
        "\n",
        "        if str(matched_row[\"Label\"].iloc[0]) == entry[2]:\n",
        "            correct_prediction_count += 1  # Increment correct_prediction_count for each correct prediction\n",
        "\n",
        "    accuracy = float(correct_prediction_count / total_count) * 100\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "d83URCQ5bb9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ROT_oP018ud"
      },
      "source": [
        "## Create model using embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCEqHBg4178A"
      },
      "outputs": [],
      "source": [
        "# Import required libraries and modules\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import CTransformers\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import os\n",
        "\n",
        "\n",
        "def textTokenizer(prompt_path):\n",
        "    # Initialize an empty list to store the text lines\n",
        "    text_lines = []\n",
        "\n",
        "    # Open the text file for reading\n",
        "    with open(prompt_path, \"r\") as file:\n",
        "        # Read each line in the file\n",
        "        for line in file:\n",
        "            # Append the line to the list of text lines\n",
        "            text_lines.append(line.strip())\n",
        "\n",
        "\n",
        "    # Create embeddings storing semantic information\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                                       model_kwargs={'device': \"cpu\"})\n",
        "\n",
        "\n",
        "    # Vectorstore for fast similarity search via indexing\n",
        "    vector_store = FAISS.from_texts(text_lines, embeddings)\n",
        "\n",
        "    return vector_store\n",
        "\n",
        "# Function to create a conversational retrieval chain model\n",
        "def createModel(prompt_path, model_name, temperature, kval, maxTokens, token):\n",
        "    vector_store = textTokenizer(prompt_path)\n",
        "\n",
        "    # Load Huggingface Llama2 LLM with specified hyperparmaters\n",
        "    llm = CTransformers(model=model_name, model_type=\"llama\", token=token,\n",
        "                        config={'max_new_tokens': maxTokens, 'temperature': temperature}, n_ctx=4096)\n",
        "\n",
        "    # Create memory object to store chat history\n",
        "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "    # Set up conversational chain that connects LLM, the indexed vectorized data, and the chatbot\n",
        "    chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm, chain_type='stuff',\n",
        "        retriever=vector_store.as_retriever(search_kwargs={\"k\": kval}),  # k hyperparameter\n",
        "        memory=memory)\n",
        "\n",
        "    return chain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Model Without Embeddings"
      ],
      "metadata": {
        "id": "4xvMZiZna7zt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4nK1XQd20Ty"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import time\n",
        "\n",
        "model_engine = \"/content/drive/MyDrive/LLama2HealthCareChatBot-master/llama-2-7b-chat.ggmlv3.q2_K.bin\"\n",
        "detail_save_folder = '/content/' # path to save the generated result\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "def model_with_embeddings(model_name, test_prompt, file_path):\n",
        "    llm = createModel('/content/drive/MyDrive/LLama2HealthCareChatBot-master/data/preprocessed.tsv', model_engine, 0.01, 2, 4096, hf_token)\n",
        "\n",
        "    # Measure the start time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Get response from the model\n",
        "    response = llm(test_prompt)\n",
        "\n",
        "    # Measure the end time\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate the time taken\n",
        "    time_taken = end_time - start_time\n",
        "\n",
        "    # Print the response and time taken\n",
        "    print(\"Model Response:\", response)\n",
        "    print(\"Time taken:\", time_taken)\n",
        "\n",
        "    # Write the prompt, model response, and time taken to the file\n",
        "    with open(file_path, 'a') as file:\n",
        "        file.write(test_prompt + response + '\\n')\n",
        "        file.write(\"Time taken: {:.2f} seconds\\n\".format(time_taken))\n",
        "\n",
        "\n",
        "def create_binding_affinity_test_prompt_1(test_list):\n",
        "    for i in range(len(test_list)):\n",
        "        prompt = \"\"\"[INST] <<SYS>>\n",
        "        You are an expert chemist, your task is to predict the binding affinity (Label), given a compound (SMILES) and given protein sequence (Sequence) using your experienced chemical property prediction knowledge. Please strictly follow the format, no other information can be provided. Please answer with one word: High, Medium, Low corresponding to binding affinity label.\n",
        "        <</SYS>>\\n\"\"\"\n",
        "        print(\"Hello\")\n",
        "        prompt += f\"SMILES: {test_list[i][0]}\\nSequence: {test_list[i][1]}\\n[/INST]\\nLabel:\\n\"\n",
        "        model_with_embeddings(model_engine, prompt, '/content/smallersamplesize_no_embed_2k.txt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import time\n",
        "\n",
        "model_engine = [\"/content/drive/MyDrive/LLama2HealthCareChatBot-master/llama-2-7b-chat.ggmlv3.q2_K.bin\", \"/content/drive/MyDrive/LLama2HealthCareChatBot-master/llama-2-7b-chat.ggmlv3.q4_0.bin\"]\n",
        "detail_save_folder = '/content/' # path to save the generated result\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "def model_without_embeddings(model_name, test_prompt, smiles, sequence, label, file_path):\n",
        "    llm = CTransformers(model=model_name, model_type=\"llama\",\n",
        "                        config={'max_new_tokens': 128, 'temperature': 0.01}, token=hf_token, n_ctx=4096)\n",
        "\n",
        "    # Measure the start time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Get response from the model\n",
        "    response = llm(test_prompt)\n",
        "\n",
        "    # Measure the end time\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate the time taken\n",
        "    time_taken = end_time - start_time\n",
        "\n",
        "    # Print the response and time taken\n",
        "    print(\"Model Response:\", response)\n",
        "    print(\"Time taken:\", time_taken)\n",
        "\n",
        "    # Write the prompt, model response, and time taken to the file\n",
        "    with open(file_path, 'a') as file:\n",
        "        file.write(test_prompt + response + '\\n')\n",
        "        file.write(\"Time taken: {:.2f} seconds\\n\".format(time_taken))\n",
        "        file.write(\"Actual Label: \" + label + \"\\n\")\n",
        "\n",
        "\n",
        "def create_binding_affinity_test_prompt_2(test_list):\n",
        "    for i in range(len(test_list)):\n",
        "        prompt = \"\"\"[INST] <<SYS>>\n",
        "        You are an expert chemist, your task is to predict the binding affinity (Label), given a compound (SMILES)\n",
        "        and given protein sequence (Sequence) using your experienced chemical property prediction knowledge.\n",
        "        Please strictly follow the format, no other information can be provided.\n",
        "        Please answer with one of the 3 labels: High OR Medium OR Low, corresponding to binding affinity label.\n",
        "        No other information can be provided.\n",
        "        <</SYS>>\\n\"\"\"\n",
        "        smiles = test_list[i][2]\n",
        "        sequence = test_list[i][6]\n",
        "        label = test_list[i][7]\n",
        "        prompt += f\"SMILES: {smiles}\\nSequence: {sequence}\\n[/INST]\\nLabel:\\n\"\n",
        "        model_without_embeddings(model_engine[0], prompt, smiles, sequence, label, '/content/smallersamplesize_no_embed_2k.txt')"
      ],
      "metadata": {
        "id": "zCb9mz9za_tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_list, test_list = create_train_test_list(20, 20)\n"
      ],
      "metadata": {
        "id": "IXh6-_PGNM1D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fc85a3f-4c88-4591-e7e7-8875a2bebf3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entering function\n",
            "Read train data\n",
            "Creating lists\n",
            "Parsing SMILES, Sequence, Label\n",
            "Successfully Parsed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_binding_affinity_test_prompt_2(test_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnekwMFF8_bN",
        "outputId": "24d245fe-ce53-4e74-c2c1-1d0cc41f5984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:ctransformers:Number of tokens (584) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (585) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (586) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (587) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (588) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (589) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (590) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (591) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (592) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (593) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (594) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (595) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (596) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (597) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (598) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (599) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (600) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (601) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (602) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (603) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (604) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (605) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (606) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (607) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (608) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (609) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (610) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (611) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (612) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (613) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (614) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (615) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (616) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (617) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (618) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (619) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (620) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (621) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (622) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (623) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (624) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (625) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (626) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (627) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (628) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (629) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (630) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (631) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (632) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (633) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (634) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (635) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (636) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (637) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (638) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (639) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (640) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (641) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (642) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (643) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (644) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (645) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (646) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (647) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (648) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (649) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (650) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (651) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (652) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (653) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (654) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (655) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (656) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (657) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (658) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (659) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (660) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (661) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (662) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (663) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (664) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (665) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (666) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (667) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (668) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (669) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (670) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (671) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (672) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (673) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (674) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (675) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (676) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (677) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (678) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (679) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (680) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (681) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (682) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (683) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (684) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (685) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (686) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (687) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (688) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (689) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (690) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (691) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (692) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (693) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (694) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (695) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (696) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (697) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (698) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (699) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (700) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (701) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (702) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (703) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (704) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (705) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (706) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (707) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (708) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (709) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (710) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (711) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (712) exceeded maximum context length (512).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Response: High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "\n",
            "Time taken: 386.18470454216003\n",
            "Model Response: High\n",
            "\n",
            "The compound you provided has a high binding affinity for the protein sequence. The molecule's structure and functional groups suggest a strong interaction with the protein, resulting in a high binding affinity.\n",
            "Time taken: 273.4255323410034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:ctransformers:Number of tokens (513) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (514) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (515) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (516) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (517) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (518) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (519) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (520) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (521) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (522) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (523) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (524) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (525) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (526) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (527) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (528) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (529) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (530) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (531) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (532) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (533) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (534) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (535) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (536) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (537) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (538) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (539) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (540) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (541) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (542) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (543) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (544) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (545) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (546) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (547) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (548) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (549) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (550) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (551) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (552) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (553) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (554) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (555) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (556) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (557) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (558) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (559) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (560) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (561) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (562) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (563) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (564) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (565) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (566) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (567) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (568) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (569) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (570) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (571) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (572) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (573) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (574) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (575) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (576) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (577) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (578) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (579) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (580) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (581) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (582) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (583) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (584) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (585) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (586) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (587) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (588) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (589) exceeded maximum context length (512).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Response: High\n",
            "\n",
            "The compound you provided has a high binding affinity for the protein sequence. The molecule's shape and size are well-suited for binding to the protein, with good hydrophobic and hydrogen bonding interactions. The molecular interactions. The molecules. The molecular interactions. The molecular interactions. The molecular interactions. The molecular interactions. The molecular interactions. The molecular interactions. The molecular interactions. The molecular interactions. The molecular interactions. The molecular interactions. The molecular interactions. The molecular interactions. The molecular interactions. The mole\n",
            "Time taken: 324.43233251571655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:ctransformers:Number of tokens (568) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (569) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (570) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (571) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (572) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (573) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (574) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (575) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (576) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (577) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (578) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (579) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (580) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (581) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (582) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (583) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (584) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (585) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (586) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (587) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (588) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (589) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (590) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (591) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (592) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (593) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (594) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (595) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (596) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (597) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (598) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (599) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (600) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (601) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (602) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (603) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (604) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (605) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (606) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (607) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (608) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (609) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (610) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (611) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (612) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (613) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (614) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (615) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (616) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (617) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (618) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (619) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (620) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (621) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (622) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (623) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (624) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (625) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (626) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (627) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (628) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (629) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (630) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (631) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (632) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (633) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (634) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (635) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (636) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (637) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (638) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (639) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (640) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (641) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (642) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (643) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (644) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (645) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (646) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (647) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (648) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (649) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (650) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (651) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (652) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (653) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (654) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (655) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (656) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (657) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (658) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (659) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (660) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (661) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (662) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (663) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (664) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (665) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (666) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (667) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (668) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (669) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (670) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (671) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (672) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (673) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (674) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (675) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (676) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (677) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (678) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (679) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (680) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (681) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (682) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (683) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (684) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (685) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (686) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (687) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (688) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (689) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (690) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (691) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (692) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (693) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (694) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (695) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (696) exceeded maximum context length (512).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Response: High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "\n",
            "Time taken: 380.25996589660645\n",
            "Model Response: High\n",
            "Time taken: 156.41199946403503\n",
            "Model Response: High\n",
            "\n",
            "The compound you provided has a high binding affinity for the protein sequence. The molecule's structure and functional groups suggest a strong interaction with the protein, resulting in a high binding affinity. The presence of aromatic rings and polar functional groups on the molecule will also contribute to this prediction.\n",
            "Time taken: 263.31946897506714\n",
            "Model Response: High\n",
            "\n",
            "Based on my knowledge of chemistry and the SMILES string you provided, I predict that this compound will have a high binding affinity for the protein sequence you provided. The presence of functional groups such as amines, carbonyls, and sulfhydryl groups, as well as the complex molecular structure of the SMILES string, suggest that this compound is likely to interact with the protein in a strong and specific manner. Additionally, the length of the sequence and the presence of multiple pharmacophore-active sites suggest that this compound will have a high binding\n",
            "Time taken: 235.28470921516418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:ctransformers:Number of tokens (513) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (514) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (515) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (516) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (517) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (518) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (519) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (520) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (521) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (522) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (523) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (524) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (525) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (526) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (527) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (528) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (529) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (530) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (531) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (532) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (533) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (534) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (535) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (536) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (537) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (538) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (539) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (540) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (541) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (542) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (543) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (544) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (545) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (546) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (547) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (548) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (549) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (550) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (551) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (552) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (553) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (554) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (555) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (556) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (557) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (558) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (559) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (560) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (561) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (562) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (563) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (564) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (565) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (566) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (567) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (568) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (569) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (570) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (571) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (572) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (573) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (574) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (575) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (576) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (577) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (578) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (579) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (580) exceeded maximum context length (512).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Response: High\n",
            "\n",
            "The compound you provided has a high binding affinity for the protein sequence MTRDEALPDSHSAQDFYENYEPKEILGRGVSSVVRRCIHKPTSQEYAVKVIDVTGGGSFSPEEVRELREATLKEVREATLKEVREATLKEVREATLKEVREATLKEVREATLKEVREATLKEVREATLKEVREATLKEVRKEVREATLKEVREATKETALKEVREATLKEVREATLKE\n",
            "Time taken: 319.6832675933838\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:ctransformers:Number of tokens (513) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (514) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (515) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (516) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (517) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (518) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (519) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (520) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (521) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (522) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (523) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (524) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (525) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (526) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (527) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (528) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (529) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (530) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (531) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (532) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (533) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (534) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (535) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (536) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (537) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (538) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (539) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (540) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (541) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (542) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (543) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (544) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (545) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (546) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (547) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (548) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (549) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (550) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (551) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (552) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (553) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (554) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (555) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (556) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (557) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (558) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (559) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (560) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (561) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (562) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (563) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (564) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (565) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (566) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (567) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (568) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (569) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (570) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (571) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (572) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (573) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (574) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (575) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (576) exceeded maximum context length (512).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Response: High\n",
            "\n",
            "The compound you provided has a strong binding affinity for the protein sequence. The SMILES string you provided represents a molecule with a high degree of polarity and aromaticity, which are desirable properties for protein-ligand interactions. Additionally, the molecule has a good hydropharmory structure that many hydropharmchelong-a number of a relatively few rotationality and acidic electrons donated rings and alkyl and efficient pi-\n",
            "a number of a high electron-a number of a reasonable hydropharmonic and polar functional groups that many hydropharm\n",
            "Time taken: 317.15400218963623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:ctransformers:Number of tokens (833) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (834) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (835) exceeded maximum context length (512).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Response: High\n",
            "Time taken: 445.0781321525574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:ctransformers:Number of tokens (513) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (514) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (515) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (516) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (517) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (518) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (519) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (520) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (521) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (522) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (523) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (524) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (525) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (526) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (527) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (528) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (529) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (530) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (531) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (532) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (533) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (534) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (535) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (536) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (537) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (538) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (539) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (540) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (541) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (542) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (543) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (544) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (545) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (546) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (547) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (548) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (549) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (550) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (551) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (552) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (553) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (554) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (555) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (556) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (557) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (558) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (559) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (560) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (561) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (562) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (563) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (564) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (565) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (566) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (567) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (568) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (569) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (570) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (571) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (572) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (573) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (574) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (575) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (576) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (577) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (578) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (579) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (580) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (581) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (582) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (583) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (584) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (585) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (586) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (587) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (588) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (589) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (590) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (591) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (592) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (593) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (594) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (595) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (596) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (597) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (598) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (599) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (600) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (601) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (602) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (603) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (604) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (605) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (606) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (607) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (608) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (609) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (610) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (611) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (612) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (613) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (614) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (615) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (616) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (617) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (618) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (619) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (620) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (621) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (622) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (623) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (624) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (625) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (626) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (627) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (628) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (629) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (630) exceeded maximum context length (512).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Response: High\n",
            "\n",
            "The compound you provided has a strong interactions with high binding affinity and interesting properties that has a strong interactions with high binding affinity and interesting chemistry and interesting chemical properties that has a strong interactions with high binding affinity and interesting properties that has a strong interactions with high binding affinity and interesting properties that has a strong interactions with high binding affinity and interesting properties that has a strong interactions with high binding affinity and interesting properties that has a strong interactions with high binding affinity and interesting chemistry and interesting chemical properties that has a strong interactions with high binding affinity and interesting properties that has a strong interactions with high\n",
            "Time taken: 358.4790961742401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:ctransformers:Number of tokens (595) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (596) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (597) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (598) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (599) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (600) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (601) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (602) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (603) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (604) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (605) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (606) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (607) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (608) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (609) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (610) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (611) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (612) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (613) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (614) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (615) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (616) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (617) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (618) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (619) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (620) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (621) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (622) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (623) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (624) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (625) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (626) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (627) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (628) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (629) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (630) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (631) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (632) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (633) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (634) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (635) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (636) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (637) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (638) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (639) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (640) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (641) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (642) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (643) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (644) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (645) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (646) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (647) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (648) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (649) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (650) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (651) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (652) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (653) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (654) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (655) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (656) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (657) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (658) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (659) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (660) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (661) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (662) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (663) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (664) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (665) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (666) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (667) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (668) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (669) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (670) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (671) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (672) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (673) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (674) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (675) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (676) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (677) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (678) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (679) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (680) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (681) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (682) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (683) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (684) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (685) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (686) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (687) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (688) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (689) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (690) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (691) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (692) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (693) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (694) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (695) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (696) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (697) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (698) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (699) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (700) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (701) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (702) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (703) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (704) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (705) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (706) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (707) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (708) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (709) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (710) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (711) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (712) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (713) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (714) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (715) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (716) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (717) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (718) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (719) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (720) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (721) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (722) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (723) exceeded maximum context length (512).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Response: High]  ]  Medium]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  Medium]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]  ]\n",
            "Time taken: 410.3279173374176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:ctransformers:Number of tokens (569) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (570) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (571) exceeded maximum context length (512).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Response: High\n",
            "Time taken: 308.883802652359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:ctransformers:Number of tokens (704) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (705) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (706) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (707) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (708) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (709) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (710) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (711) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (712) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (713) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (714) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (715) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (716) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (717) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (718) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (719) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (720) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (721) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (722) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (723) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (724) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (725) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (726) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (727) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (728) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (729) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (730) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (731) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (732) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (733) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (734) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (735) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (736) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (737) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (738) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (739) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (740) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (741) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (742) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (743) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (744) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (745) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (746) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (747) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (748) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (749) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (750) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (751) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (752) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (753) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (754) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (755) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (756) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (757) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (758) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (759) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (760) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (761) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (762) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (763) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (764) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (765) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (766) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (767) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (768) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (769) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (770) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (771) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (772) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (773) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (774) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (775) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (776) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (777) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (778) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (779) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (780) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (781) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (782) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (783) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (784) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (785) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (786) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (787) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (788) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (789) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (790) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (791) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (792) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (793) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (794) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (795) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (796) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (797) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (798) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (799) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (800) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (801) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (802) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (803) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (804) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (805) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (806) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (807) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (808) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (809) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (810) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (811) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (812) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (813) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (814) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (815) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (816) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (817) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (818) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (819) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (820) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (821) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (822) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (823) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (824) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (825) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (826) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (827) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (828) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (829) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (830) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (831) exceeded maximum context length (512).\n",
            "WARNING:ctransformers:Number of tokens (832) exceeded maximum context length (512).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Response: High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "High\n",
            "\n",
            "Time taken: 452.21503043174744\n",
            "Model Response: High\n",
            "\n",
            "The compound you provided has a high binding affinity for the protein sequence. The molecule's shape and size are compatible with the active site of the protein, and its functional groups are well-positioned to interact with key residues on the protein surface. These factors contribute to a strong binding affinity.\n",
            "Time taken: 246.44161677360535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:ctransformers:Number of tokens (1063) exceeded maximum context length (512).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ofr0sUdvRro3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}